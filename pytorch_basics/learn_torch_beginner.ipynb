{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor**\n",
        "\n",
        "Similar to Numpy ndarrays. Optimized for automatic differentiation."
      ],
      "metadata": {
        "id": "JyqZz2CiK8BB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNkmw32VKj3O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "data = [[1, 2], [3, 4]]\n",
        "# From python array\n",
        "x_data = torch.tensor(data)\n",
        "# From another tensor\n",
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)"
      ],
      "metadata": {
        "id": "oWd0BxEoLcZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Operations\n",
        "\n",
        "# To device\n",
        "if torch.cuda.is_available():\n",
        "    tensor = rand_tensor.to(\"cuda\")\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")\n",
        "\n",
        "# Deep Copy\n",
        "# Slices are views (not copies), so changes affect the original array, use .clone().detach()\n",
        "tensor_copy = tensor.clone().detach()\n",
        "\n",
        "# Index and slice\n",
        "# syntax: start(inc):end(exc):step. For multi-dimensional arrays, slicing is done per axis.\n",
        "tensor[:,1] = 0 # This combines slice and index: [slice all rows, index 1]\n",
        "\n",
        "print(tensor)\n",
        "print(tensor_copy)"
      ],
      "metadata": {
        "id": "1_TcWQCzMmii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate a sequence of tensors along a given dimension, 0 = row 1 = col\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=0)\n",
        "print(t1)"
      ],
      "metadata": {
        "id": "tpIXai5AM7g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arithmetic operations: y1, y2, y3 will have the same value\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eClb7evvUiy8",
        "outputId": "00ed94cd-f0e6-4f03-97c6-80f09412d9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9383, 0.0000, 0.7632],\n",
              "        [0.3839, 0.0000, 0.4835]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agg = tensor.sum()\n",
        "agg_item = agg.item()"
      ],
      "metadata": {
        "id": "uNsFFZKeVXmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Datasets & DataLoaders**\n",
        "\n",
        " PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n",
        "\n",
        "\n",
        "**Transform Data**\n",
        "All TorchVision datasets have two parameters -transform to modify the features and target_transform to modify the labels - that accept callables containing the transformation logic.\n",
        "\n",
        "The FashionMNIST features are in PIL Image format, and the labels are integers. For training, we need the features as **normalized tensors**, and the labels as **one-hot encoded tensors**. To make these transformations, we use ToTensor and Lambda."
      ],
      "metadata": {
        "id": "g215pnSLWNAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "# ToTensor Convert a PIL Image or ndarray to tensor and scale the values accordingly.\n",
        "# Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255]\n",
        "# to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "img, label = training_data[sample_idx]\n",
        "img.shape # C=1, H=28, W=28 Number of channels (feature dimensions). Here grayscale, so C dimension is 1. RGB: C=3.\n",
        "plt.title(labels_map[label])\n",
        "plt.axis(\"off\")\n",
        "# squeeze() removes dimensions of size 1 from an array.\n",
        "# For example, if your image array has a shape like (1, 224, 224, 3), applying squeeze() will result in an array with shape (224, 224, 3).\n",
        "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        ")"
      ],
      "metadata": {
        "id": "PucMjqvJVxLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, **after we iterate over all batches the data is shuffled**"
      ],
      "metadata": {
        "id": "7TOM4LkRuHYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
        "\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "\n",
        "print(f\"Feature batch shape: {train_features.size()}\") # B, C, H, W\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOWpqR1FpEI4",
        "outputId": "0e96caa5-ecfa-4c7e-acc5-16674dcd9324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Neural Network\n",
        "\n",
        "torch.nn has all the building blocks needed. Every module is a subclass of nn.Module. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily."
      ],
      "metadata": {
        "id": "H7q0jFaM08cR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Linear + Relu common practice\n",
        "# ReLU introduces sparsity in the activations\n",
        "# ReLU simplifies the optimization landscape by introducing piecewise linearity, which helps gradient-based optimizers converge faster compared to smoother activation functions like sigmoid.\n",
        "# ReLU (Rectified Linear Unit) introduces sparsity into neural networks by selectively activating only a subset of neurons in each layer\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten() #  convert each 2D 28x28 image into a contiguous array of 784 pixel values\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(), # ReLU Introduces Non-Linearity. Without non-linearity, stacking multiple linear layers is equivalent to a single linear transformation\n",
        "            nn.Linear(512, 512), # 512 In essence, 512: dimensionality the hidden layers in the neural network.\n",
        "            nn.ReLU(), # ReLU avoids the vanishing gradient problem, which is common with activation functions like sigmoid and tanh\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMYX9bu7zPlW",
        "outputId": "648f738f-e1b6-46c2-e8a8-14b4d953bcd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_features[0].to(device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1) (logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NWBan4Et-h1",
        "outputId": "5cd7662f-028d-4986-ec04-cc68f036590b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([1], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "metadata": {
        "id": "iDALn3C_6MVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Autograd**\n",
        "\n",
        "To compute those gradients, PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph. for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code `with torch.no_grad()` block:\n",
        "\n",
        "There are reasons you might want to disable gradient tracking:\n",
        "- To mark some parameters in your neural network as frozen parameters.\n",
        "- To speed up computations when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.\n",
        "\n",
        "**DAGs are dynamic**\n",
        "in PyTorch An important thing to note is that the graph is recreated from scratch; **after each .backward() call**, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.\n",
        "\n",
        "\n",
        "PyTorch accumulates the gradients, i.e. the value of computed gradients is added to the grad property of all leaf nodes of computational graph. If you want to compute the proper gradients, you need to zero out the grad property before. In real-life training an optimizer helps us to do this.\n",
        "\n"
      ],
      "metadata": {
        "id": "ERGGe-kvAMvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizing Model Parameters\n",
        "\n"
      ],
      "metadata": {
        "id": "vkIa6UUGCvkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        # After flattnen, the input can be 64 * 784 where B=64, Dimension=784\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ],
      "metadata": {
        "id": "-Jg5mrJM78bO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparams\n",
        "\n",
        "adjustable parameters that let you control the model optimization process:\n",
        "- Number of Epochs - the number times to iterate over the dataset\n",
        "- Batch Size - the number of data samples propagated through the network before the parameters are updated\n",
        "- Learning Rate - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
      ],
      "metadata": {
        "id": "2qwC8r-_Grkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "# Regression: MSE, NLL Classificaion: CrossEntropy\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch_num, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward() # Calculates the gradients of the loss with respect to the model's parameters.\n",
        "        optimizer.step() # Updates the model's parameters based on the calculated gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch_num % 100 == 0:\n",
        "            loss, current = loss.item(), (batch_num + 1) * batch_size\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "train_loop(train_dataloader, model, loss_fn, optimizer)"
      ],
      "metadata": {
        "id": "sUVAqJplGc3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0014c2b1-4d97-4dc3-f94e-29048e329a67"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.675389  [   64/60000]\n",
            "loss: 1.643331  [ 6464/60000]\n",
            "loss: 1.502533  [12864/60000]\n",
            "loss: 1.585111  [19264/60000]\n",
            "loss: 1.454386  [25664/60000]\n",
            "loss: 1.435264  [32064/60000]\n",
            "loss: 1.456058  [38464/60000]\n",
            "loss: 1.368090  [44864/60000]\n",
            "loss: 1.400835  [51264/60000]\n",
            "loss: 1.310243  [57664/60000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside the training loop, optimization happens in three steps:\n",
        "Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
        "\n",
        "Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
        "\n",
        "Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
      ],
      "metadata": {
        "id": "7qvZ-dRCH-1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "IoNwHaEIfji3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient is a special case of the Jacobian matrix when m=1."
      ],
      "metadata": {
        "id": "kwIk4pFtfNrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "NfARIezzfNHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch models store the learned parameters in an internal state dictionary, called state_dict. These can be persisted via the torch.save method.\n",
        "\n",
        "be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results."
      ],
      "metadata": {
        "id": "akgUyXTZgm0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model.pth')\n",
        "model = torch.load('model.pth')"
      ],
      "metadata": {
        "id": "XvWk_2O1hLiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MA1Dhj0shtY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}